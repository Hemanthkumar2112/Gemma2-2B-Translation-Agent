{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q git+https://github.com/huggingface/transformers.git\n!pip install -q git+https://github.com/huggingface/peft.git\n!pip install -q git+https://github.com/huggingface/accelerate.git@main\n!pip install bitsandbytes SentencePiece","metadata":{"id":"jONLwzXgLg-I","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import PeftModel\nimport torch\nfrom transformers import pipeline\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n!huggingface-cli login --token  replace_this_to_huggingface_token","metadata":{"id":"36460935","execution":{"iopub.status.busy":"2024-09-21T14:20:02.786144Z","iopub.execute_input":"2024-09-21T14:20:02.786496Z","iopub.status.idle":"2024-09-21T14:20:12.804756Z","shell.execute_reply.started":"2024-09-21T14:20:02.786466Z","shell.execute_reply":"2024-09-21T14:20:12.803714Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-09-21 14:20:07.257301: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-09-21 14:20:07.257353: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-09-21 14:20:07.258734: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"base_model = \"google/gemma-2-2b\"\n# PEFT models paths\npeft_models_path = {\n    'hindi': \"Hemanth-thunder/model_gemma2b_mt-hi_1k_steps\",\n    'tamil': \"Hemanth-thunder/model_gemma2b_mt-ta_2k_steps\",\n    'kannada': \"Hemanth-thunder/model_gemma2b_mt-kh_10k_steps\",\n    'malayalam': \"Hemanth-thunder/model_gemma2b_mt-ml_10k_steps\",\n    'telugu': \"Hemanth-thunder/model_gemma2b_mt-te_10k_steps\",\n    'bengali': \"Hemanth-thunder/model_gemma2b_mt-bn_10k_steps\",\n    'marathi': \"Hemanth-thunder/model_gemma2b_mt-mr_10k_steps\",\n    'gujarati': \"Hemanth-thunder/model_gemma2b_mt-gujarati_gu_10k_steps\",\n    'odia': \"Hemanth-thunder/model_gemma2b_mt-odia_or_10k_steps\"\n}\n\n# Adapter names\nadapter_names = list(peft_models_path.keys())\n\npipeline_type = \"text-generation\"\n\nalpaca_prompt = \"\"\"\n### Instruction:\nYour are an AI translator to translate the following English input sentence to {} sentence.\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T14:20:31.029825Z","iopub.execute_input":"2024-09-21T14:20:31.030193Z","iopub.status.idle":"2024-09-21T14:20:31.036573Z","shell.execute_reply.started":"2024-09-21T14:20:31.030164Z","shell.execute_reply":"2024-09-21T14:20:31.035485Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"## function for generatining text \ndef generate(lang, english_text, response_text, merged_model, to_set_lang):\n    # Prepare the prompt using the language and texts\n    prompt = alpaca_prompt.format(lang.capitalize(), english_text, response_text)\n\n    # Set the adapter for the merged model if specified\n    if to_set_lang:\n        merged_model.set_adapter(lang)\n        print(f\"adaptor switched to {lang}\")\n\n    # Tokenize the input prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    # Generate output using the model\n    try:\n        outputs = merged_model.generate(**inputs, max_new_tokens=256,do_sample=True,top_p=0.95,temperature=0.2,repetition_penalty=1.2,eos_token_id=tokenizer.eos_token_id,)\n        # Decode and return the response\n        response = tokenizer.decode(outputs[0])\n        return response.split(\"### Response:\")[-1].strip()\n    \n    except Exception as e:\n        print(f\"Error during generation: {e}\")\n        return None","metadata":{"execution":{"iopub.status.busy":"2024-09-21T14:22:30.137808Z","iopub.execute_input":"2024-09-21T14:22:30.138697Z","iopub.status.idle":"2024-09-21T14:22:30.145932Z","shell.execute_reply.started":"2024-09-21T14:22:30.138652Z","shell.execute_reply":"2024-09-21T14:22:30.144913Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_quant_type=\"nf4\",bnb_4bit_compute_dtype=torch.bfloat16,bnb_4bit_use_double_quant=True,)\nmodel = AutoModelForCausalLM.from_pretrained(base_model, device_map={\"\": 0},quantization_config=bnb_config)\ntokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f0f515ed","outputId":"312488a5-f4f8-48a4-8c63-7b4a59e80418","execution":{"iopub.status.busy":"2024-09-21T14:22:33.856098Z","iopub.execute_input":"2024-09-21T14:22:33.856522Z","iopub.status.idle":"2024-09-21T14:22:41.664852Z","shell.execute_reply.started":"2024-09-21T14:22:33.856490Z","shell.execute_reply":"2024-09-21T14:22:41.663825Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da1ff6c3a4464778bfdfa53894855bc1"}},"metadata":{}}]},{"cell_type":"code","source":"### stack other language with the peft model\n#### merging tamil translation with base model with the adapter name tamil\n\npeft_model = PeftModel.from_pretrained(model, peft_models_path['tamil'], adapter_name=\"tamil\")\n\nfor lang, path in peft_models_path.items():\n    if lang != \"tamil\":\n        try:\n            _ = peft_model.load_adapter(path, adapter_name=lang)\n            print(f\"load {lang}\")\n        except Exception as e:\n            print(f\"Error loading adapter for {lang}: {e}\")\n# Set the model to evaluation mode\npeft_model.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n### testing # lang,english_text,response_text,merged_model\ntext = \"Heavy rain has caused widespread flooding in several parts of the country.\"\n\nfor lang in adapter_names:\n    inference_result = generate(lang,text,\"\",peft_model,to_set_lang=True)\n    print(f\"inference_result for {lang} : {inference_result}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-21T14:24:43.200094Z","iopub.execute_input":"2024-09-21T14:24:43.200521Z","iopub.status.idle":"2024-09-21T14:25:34.456243Z","shell.execute_reply.started":"2024-09-21T14:24:43.200487Z","shell.execute_reply":"2024-09-21T14:25:34.455232Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n","output_type":"stream"},{"name":"stdout","text":"adaptor switched to hindi\n","output_type":"stream"},{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"inference_result for hindi : गंभीर वर्षा ने देश के कई भागों में बाढ़ की स्थिति उत्पन्न कर दी है।<eos>\nadaptor switched to tamil\ninference_result for tamil : நாட்டின் பலப் பகுதிகளில் கனமழை பெய்துள்ளது, அதன் விளைவாக ஏராளமான இடங்களிலும் வெள்ளம் ஏற்பட்டுவிட்டது.\n<eos>\nadaptor switched to kannada\ninference_result for kannada : ದೇಶಾದ್ಯಂತ ಮಳೆಗೆ ತೀವ್ರ ಪ್ರಭಾವ ಬಂದಿದೆ, ಹಲವು ಜಿಲ್ಲೆಯಲ್ಲಿ ಅಪಾಯಕಾರಿ ಸ್ಥಿತಿಯುಂಟಾಗಿದ್ದಾರೆ.<eos>\nadaptor switched to malayalam\ninference_result for malayalam : രാജ്യത്തെ പലയിടങ്ങളിലും കനത്ത മഴയുടെ ഭീഷണി ഉണ്ടാക്കുന്നുണ്ട്.<eos>\nadaptor switched to telugu\ninference_result for telugu : చాలా ప్రాంతాల్లో భారీ వర్షం కారణంగా చుట్టూ నీళ్ళతో నిండిపోయింది.\n<eos>\nadaptor switched to bengali\ninference_result for bengali : কয়েকটি অঞ্চলে বৃষ্টিপাতের ফলস্বরূপ প্রায় সব জেলায় পানিধারা দেখা দিয়েছে।<eos>\nadaptor switched to marathi\ninference_result for marathi : अनेक ठिकाणी पावसाच्या सततधारामुळे पूरस्थिती निर्माण झाली आहे.<eos>\nadaptor switched to gujarati\ninference_result for gujarati : દેશના અનેક વિસ્તારોમાં ભારે વરસાદ પડ્યો છે, જેથી કેટલાંક જગ્યાઓ પર પાણીની ધરાવ થઈ હતી.<eos>\nadaptor switched to odia\ninference_result for odia : ବହୁତ କମ୍ପାସିଂରେ ଦୀର୍ଘଗତ ଜଳବନ୍ଧଣ ଭୂମିକୁ ଅଛି ।<eos>\nCPU times: user 51.1 s, sys: 202 ms, total: 51.3 s\nWall time: 51.2 s\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}